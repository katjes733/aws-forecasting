{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda update -n base -c conda-forge conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install bokeh=2.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pandas_bokeh=0.5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# importing forecast notebook utility from notebooks/common directory\n",
    "sys.path.insert(0, os.path.abspath(\"./common/\"))\n",
    "import util\n",
    "import util.fcst_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (15.0, 5.0)\n",
    "\n",
    "import pandas_bokeh\n",
    "pandas_bokeh.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup variables\n",
    "owner = \"martin.macecek@rearc.io\"\n",
    "type = \"Internal\"\n",
    "usage = \"Playground\"\n",
    "prefix = \"mac-training\"\n",
    "role_name = f\"{prefix}-forecasting\"\n",
    "bucket_name = f\"{prefix}-bucket-275279264324-us-east-1\"\n",
    "data_key = \"forecasting/input/RIVN.csv\"\n",
    "prepared_data_key_prefix = \"forecasting/prepared/rivn\"\n",
    "item_id = \"RIVN\"\n",
    "target_column_name = \"close\"\n",
    "\n",
    "# Setup more variables\n",
    "s3_target_data_key = f\"s3://{bucket_name}/{prepared_data_key_prefix}.csv\"\n",
    "s3_related_data_key = f\"s3://{bucket_name}/{prepared_data_key_prefix}_rts.csv\"\n",
    "date_format = '%Y%m%d_%H%M%S'\n",
    "ui_date_format = '%a, %d %b %Y %H:%M:%S %Z'\n",
    "\n",
    "# Tags for resource tagging\n",
    "tags = [{'Key': 'Owner', 'Value': owner},\n",
    "        {'Key': 'Type', 'Value': type},\n",
    "        {'Key': 'Usage', 'Value': usage}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "print(f\"Account: {account_id}, Region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect API sessions\n",
    "session = boto3.Session(region_name=region) \n",
    "s3 = session.client(service_name='s3')\n",
    "forecast = session.client(service_name='forecast') \n",
    "forecastquery = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or retrieve the role to provide to Amazon Forecast.\n",
    "role_arn = util.get_or_create_iam_role(role_name=role_name)\n",
    "\n",
    "# echo user inputs without account\n",
    "print(f\"Success! Role '{role_arn.split('/')[1]}' ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_or_create_bucket(bucket_name, region=region)\n",
    "print(f\"Success! Bucket '{bucket_name}' ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = pd.read_csv(f\"s3://{bucket_name}/{data_key}\", dtype=object)\n",
    "stock_df.drop([\"Vol.\", \"Change %\"], axis=1, inplace=True)\n",
    "stock_df[\"Date\"] = pd.to_datetime(stock_df[\"Date\"], format=\"%m/%d/%Y\").dt.date\n",
    "stock_df[[\"Price\", \"Open\", \"High\", \"Low\"]] = stock_df[[\"Price\", \"Open\", \"High\", \"Low\"]].astype(float)\n",
    "stock_df.rename(columns={'Date': 'datetime', 'Price': target_column_name, 'Open': 'open', 'High': 'high', 'Low': 'low'}, inplace=True)\n",
    "stock_df[\"item_id\"] = item_id\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index = pd.Index(pd.date_range(stock_df.datetime.min(), stock_df.datetime.max()), name=\"datetime\")\n",
    "stock_df.set_index(\"datetime\").reindex(new_index)\n",
    "stock_df = stock_df.set_index(\"datetime\").reindex(new_index).reset_index().ffill()\n",
    "stock_df = stock_df.sort_values(by=['datetime'], ascending=False)\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.plot(x='datetime', y=[target_column_name, 'open', 'high', 'low'], figsize=(15, 8))\n",
    "plt.xlabel('Date Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare and Save the Target Time Series\n",
    "\n",
    "In this exmple, we are only using the supplemental fields `open`, `high` and `low` for forecasting.\n",
    "Later we may explore with supplemental values from other stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast length in days (Units is defined below)\n",
    "FORECAST_LENGTH = 30\n",
    "\n",
    "# What is your forecast time unit granularity?\n",
    "# Choices are: ^Y|M|W|D|h|30min|15min|10min|5min|1min$ \n",
    "DATASET_FREQUENCY = \"D\"\n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd\"\n",
    "# delimiter = ','\n",
    "\n",
    "# What name do you want to give this project?  \n",
    "# We will use this same name for your Forecast Dataset Group name.\n",
    "PROJECT = 'rivn-forecast'\n",
    "DATA_VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = stock_df[['item_id', 'datetime', target_column_name]][:-FORECAST_LENGTH]\n",
    "target_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_df = stock_df[['item_id', 'datetime', 'open', 'high', 'low']]\n",
    "rts_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(target_df)} + {FORECAST_LENGTH} = {len(rts_df)}\")\n",
    "assert len(target_df) + FORECAST_LENGTH == len(rts_df), \"length doesn't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.to_csv(s3_target_data_key, index= False, header = False)\n",
    "rts_df.to_csv(s3_related_data_key, index= False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create the Dataset Group and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_group_name = f\"{prefix}_{PROJECT}_{DATA_VERSION}\".replace(\"-\", \"_\")\n",
    "print(f\"Dataset Group Name = {dataset_group_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arns = []\n",
    "try:\n",
    "    create_dataset_group_response = \\\n",
    "        forecast.create_dataset_group(Domain=\"RETAIL\",\n",
    "                                      DatasetGroupName=dataset_group_name,\n",
    "                                      DatasetArns=dataset_arns,\n",
    "                                      Tags=tags\n",
    "                                     )\n",
    "    dataset_group_arn = create_dataset_group_response['DatasetGroupArn']\n",
    "    status = util.wait(lambda: forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn))\n",
    "    assert status\n",
    "except forecast.exceptions.ResourceAlreadyExistsException:\n",
    "    dataset_group_arn = f\"arn:aws:forecast:{region}:{account_id}:dataset-group/{dataset_group_name}\"\n",
    "    print(f\"Dataset group {dataset_group_arn} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the schema of your dataset here. Make sure the order of columns matches the raw data files.\n",
    "tts_schema = {\n",
    "   \"Attributes\": [\n",
    "      {\n",
    "         \"AttributeName\": \"item_id\",\n",
    "         \"AttributeType\": \"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\": \"timestamp\",\n",
    "         \"AttributeType\": \"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\": \"demand\",\n",
    "         \"AttributeType\": \"float\"\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_dataset_name = f\"{dataset_group_name}_tts\"\n",
    "print(tts_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_dataset_tts_response = \\\n",
    "        forecast.create_dataset(Domain=\"RETAIL\",\n",
    "                                DatasetType='TARGET_TIME_SERIES',\n",
    "                                DatasetName=tts_dataset_name,\n",
    "                                DataFrequency=DATASET_FREQUENCY,\n",
    "                                Schema=tts_schema,\n",
    "                                Tags=tags\n",
    "                               )\n",
    "    tts_dataset_arn = create_dataset_tts_response['DatasetArn']\n",
    "    status = util.wait(lambda: forecast.describe_dataset(DatasetArn=tts_dataset_arn))\n",
    "    assert status\n",
    "except forecast.exceptions.ResourceAlreadyExistsException:\n",
    "    tts_dataset_arn = f\"arn:aws:forecast:{region}:{account_id}:dataset/{tts_dataset_name}\"\n",
    "    print(f\"Target dataset {tts_dataset_arn} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the schema of your dataset here. Make sure the order of columns matches the raw data files.\n",
    "rts_schema = {\n",
    "   \"Attributes\": [\n",
    "      {\n",
    "         \"AttributeName\": \"item_id\",\n",
    "         \"AttributeType\": \"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\": \"timestamp\",\n",
    "         \"AttributeType\": \"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\": \"open_price\",\n",
    "         \"AttributeType\": \"float\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\": \"high_price\",\n",
    "         \"AttributeType\": \"float\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\": \"low_price\",\n",
    "         \"AttributeType\": \"float\"\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_dataset_name = f\"{dataset_group_name}_rts\"\n",
    "print(rts_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_dataset_rts_response = \\\n",
    "        forecast.create_dataset(Domain=\"RETAIL\",\n",
    "                                DatasetType='RELATED_TIME_SERIES',\n",
    "                                DatasetName=rts_dataset_name,\n",
    "                                DataFrequency=DATASET_FREQUENCY,\n",
    "                                Schema=rts_schema,\n",
    "                                Tags=tags\n",
    "                               )\n",
    "    rts_dataset_arn = create_dataset_rts_response['DatasetArn']\n",
    "    status = util.wait(lambda: forecast.describe_dataset(DatasetArn=rts_dataset_arn))\n",
    "    assert status\n",
    "except forecast.exceptions.ResourceAlreadyExistsException:\n",
    "    rts_dataset_arn = f\"arn:aws:forecast:{region}:{account_id}:dataset/{rts_dataset_name}\"\n",
    "    print(f\"Related dataset {rts_dataset_arn} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arns = []\n",
    "dataset_arns.append(tts_dataset_arn)\n",
    "dataset_arns.append(rts_dataset_arn)\n",
    "update_dataset_response = forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=dataset_arns)\n",
    "status = util.wait(lambda: forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Import data from S3 to Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Depending on the data size, the import can take 10 mins or more to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_tts_dataset_response = True\n",
    "if len(util.get_dataset_import_jobs(tts_dataset_arn, forecast)) > 0:\n",
    "    print(\"Target dataset has already imported data.\")\n",
    "    import_tts_dataset_response = True if input(\"Re-import (y/N)? \").lower() == \"y\" else False\n",
    "\n",
    "if import_tts_dataset_response:\n",
    "    tts_dataset_import_job_response = \\\n",
    "        forecast.create_dataset_import_job(DatasetImportJobName=f\"tts_job_{datetime.now().strftime(date_format)}\",\n",
    "                                           DatasetArn=tts_dataset_arn,\n",
    "                                           DataSource= {\n",
    "                                             \"S3Config\" : {\n",
    "                                                 \"Path\": s3_target_data_key,\n",
    "                                                 \"RoleArn\": role_arn\n",
    "                                             } \n",
    "                                           },\n",
    "                                           TimestampFormat=TIMESTAMP_FORMAT,\n",
    "                                           Tags=tags\n",
    "                                          )\n",
    "    tts_dataset_import_job_arn=tts_dataset_import_job_response['DatasetImportJobArn']\n",
    "    status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=tts_dataset_import_job_arn))\n",
    "    assert status\n",
    "    print(\"Target data imported.\")\n",
    "else:\n",
    "    print(\"Target data re-import skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Depending on the data size, the import can take 10 mins or more to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_rts_dataset_response = True\n",
    "if len(util.get_dataset_import_jobs(rts_dataset_arn, forecast)) > 0:\n",
    "    print(\"Related dataset has already imported data.\")\n",
    "    import_rts_dataset_response = True if input(\"Re-import (y/N)? \").lower() == \"y\" else False\n",
    "\n",
    "if import_rts_dataset_response:\n",
    "    rts_dataset_import_job_response = \\\n",
    "        forecast.create_dataset_import_job(DatasetImportJobName=f\"rts_job_{datetime.now().strftime(date_format)}\",\n",
    "                                           DatasetArn=rts_dataset_arn,\n",
    "                                           DataSource= {\n",
    "                                             \"S3Config\" : {\n",
    "                                                 \"Path\": s3_related_data_key,\n",
    "                                                 \"RoleArn\": role_arn\n",
    "                                             } \n",
    "                                           },\n",
    "                                           TimestampFormat=TIMESTAMP_FORMAT,\n",
    "                                           Tags=tags\n",
    "                                          )\n",
    "    rts_dataset_import_job_arn = rts_dataset_import_job_response[\"DatasetImportJobArn\"]\n",
    "    status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=rts_dataset_import_job_arn))\n",
    "    assert status\n",
    "    print(\"Related data imported.\")\n",
    "else:\n",
    "    print(\"Related data re-import skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn = 'arn:aws:forecast:::algorithm/'\n",
    "algorithm = 'Deep_AR_Plus'\n",
    "algorithm_arn_deep_ar_plus = algorithm_arn + algorithm\n",
    "predictor_name_deep_ar = f\"{dataset_group_name}_{algorithm.lower()}\"\n",
    "print(f\"Predictor Name = {predictor_name_deep_ar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Training a forecast model can take several hours to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_predictor_deep_ar = False\n",
    "existing_predictor_deep_ar = util.get_predictor(predictor_name_deep_ar, forecast)\n",
    "\n",
    "if existing_predictor_deep_ar:\n",
    "    predictor_arn_deep_ar = existing_predictor_deep_ar['PredictorArn']\n",
    "    print(f\"DeepAR+ Predictor {predictor_arn_deep_ar} already exists.\")\n",
    "    retrain_predictor_deep_ar = True if input(\"Retrain model (y/N)? \").lower() == \"y\" else False\n",
    "\n",
    "if existing_predictor_deep_ar and retrain_predictor_deep_ar:\n",
    "    util.delete_forecasts_by_predictor(predictor_arn_deep_ar, forecast)\n",
    "    print(f\"Deleting DeepAR+ Predictor {predictor_arn_deep_ar}...\")\n",
    "    util.wait_till_delete(lambda: forecast.delete_predictor(PredictorArn=predictor_arn_deep_ar))\n",
    "elif existing_predictor_deep_ar and not retrain_predictor_deep_ar:\n",
    "    print(f\"Keeping existing DeepAR+ Predictor {predictor_arn_deep_ar}.\")\n",
    "\n",
    "if not existing_predictor_deep_ar or retrain_predictor_deep_ar:\n",
    "    print(f\"Creating DeepAR+ Predictor {predictor_arn_deep_ar}...\")\n",
    "    create_predictor_deep_ar_response = \\\n",
    "        forecast.create_predictor(PredictorName=predictor_name_deep_ar,\n",
    "                                  AlgorithmArn=algorithm_arn_deep_ar_plus,\n",
    "                                  ForecastHorizon=FORECAST_LENGTH,\n",
    "                                  PerformAutoML=False,\n",
    "                                  PerformHPO=False,\n",
    "                                  InputDataConfig={\n",
    "                                      \"DatasetGroupArn\": dataset_group_arn,\n",
    "                                      \"SupplementaryFeatures\": [\n",
    "                                          {\"Name\": \"holiday\",\n",
    "                                           \"Value\": \"US\"}],\n",
    "                                  },\n",
    "                                  FeaturizationConfig={\"ForecastFrequency\": DATASET_FREQUENCY},\n",
    "                                  Tags=tags\n",
    "                                 )\n",
    "    predictor_arn_deep_ar = create_predictor_deep_ar_response['PredictorArn']\n",
    "    status = util.wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_deep_ar))\n",
    "    assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn = 'arn:aws:forecast:::algorithm/'\n",
    "algorithm = 'Prophet'\n",
    "algorithm_arn_prophet = algorithm_arn + algorithm\n",
    "predictor_name_prophet = f\"{dataset_group_name}_{algorithm.lower()}\"\n",
    "print(f\"Predictor Name = {predictor_name_prophet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Training a forecast model can take several hours to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_predictor_prophet = False\n",
    "existing_predictor_prophet = util.get_predictor(predictor_name_prophet, forecast)\n",
    "\n",
    "if existing_predictor_prophet:\n",
    "    predictor_arn_prophet = existing_predictor_prophet['PredictorArn']\n",
    "    print(f\"Prophet Predictor {predictor_arn_prophet} already exists.\")\n",
    "    retrain_predictor_prophet = True if input(\"Retrain model (y/N)? \").lower() == \"y\" else False\n",
    "\n",
    "if existing_predictor_prophet and retrain_predictor_prophet:\n",
    "    print(f\"Deleting Prophet Predictor {predictor_arn_prophet}...\")\n",
    "    util.wait_till_delete(lambda: forecast.delete_predictor(PredictorArn=predictor_arn_prophet))\n",
    "elif existing_predictor_prophet and not retrain_predictor_prophet:\n",
    "    print(f\"Keeping existing Prophet Predictor {predictor_arn_prophet}.\")\n",
    "\n",
    "if not existing_predictor_prophet or retrain_predictor_prophet:\n",
    "    print(f\"Creating Prophet Predictor {predictor_arn_prophet}...\")\n",
    "    create_predictor_response = \\\n",
    "        forecast.create_predictor(PredictorName=predictor_name_prophet,\n",
    "                                  AlgorithmArn=algorithm_arn_prophet,\n",
    "                                  ForecastHorizon=FORECAST_LENGTH,\n",
    "                                  PerformAutoML=False,\n",
    "                                  PerformHPO=False,\n",
    "                                  InputDataConfig= {\n",
    "                                      \"DatasetGroupArn\": dataset_group_arn,\n",
    "                                      \"SupplementaryFeatures\": [\n",
    "                                          {\"Name\": \"holiday\",\n",
    "                                           \"Value\": \"US\"}],\n",
    "                                  },\n",
    "                                  FeaturizationConfig= {\"ForecastFrequency\": DATASET_FREQUENCY},\n",
    "                                  Tags=tags\n",
    "                                 )\n",
    "    predictor_arn_prophet = create_predictor_response['PredictorArn']\n",
    "    status = util.wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_prophet))\n",
    "    assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_name_auto = f\"{dataset_group_name}_auto\"\n",
    "print(f\"Predictor Name = {predictor_name_auto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Training a forecast model can take several hours to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_predictor_auto = util.get_predictor(predictor_name_auto, forecast)\n",
    "if existing_predictor_auto:\n",
    "    predictor_arn_auto = existing_predictor_auto['PredictorArn']\n",
    "    print(f\"Auto Predictor {predictor_arn_auto} already exists.\")\n",
    "    if input(\"Retrain model (y/N)? \").lower() == \"y\":\n",
    "        args = {\n",
    "            \"PredictorName\": f\"{predictor_name_auto}_retrain_{datetime.now().strftime(date_format)}\",\n",
    "            \"ReferencePredictorArn\": predictor_arn_auto\n",
    "        }\n",
    "    else:\n",
    "        args = {}\n",
    "else:\n",
    "    args = {\n",
    "        \"PredictorName\": predictor_name_auto,\n",
    "        \"ForecastHorizon\": FORECAST_LENGTH,\n",
    "        \"ForecastFrequency\": DATASET_FREQUENCY,\n",
    "        \"DataConfig\": {\n",
    "            \"DatasetGroupArn\": dataset_group_arn,\n",
    "            \"AdditionalDatasets\": [\n",
    "                {\n",
    "                    \"Name\": \"holiday\",\n",
    "                    \"Configuration\": {\n",
    "                        \"CountryCode\": [\"US\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"Tags\": tags\n",
    "    }\n",
    "\n",
    "if args:\n",
    "    create_predictor_auto_response = forecast.create_auto_predictor(**args)\n",
    "    predictor_arn_auto = create_predictor_auto_response['PredictorArn']\n",
    "    print(f\"{'Retraining existing' if existing_predictor_auto else 'Creating new'} Auto Predictor {predictor_arn_auto}...\")\n",
    "    status = util.wait(lambda: forecast.describe_auto_predictor(PredictorArn=predictor_arn_auto))\n",
    "    assert status\n",
    "else:\n",
    "    print(f\"Keeping existing Auto Predictor {predictor_arn_auto}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Predictor Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_deep_ar_plus = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_deep_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_prophet = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_auto = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_metrics(metric_response, predictor_name):\n",
    "    df = pd.DataFrame(metric_response['PredictorEvaluationResults']\n",
    "                 [0]['TestWindows'][0]['Metrics']['WeightedQuantileLosses'])\n",
    "    df['Predictor'] = predictor_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_ar_metrics = extract_summary_metrics(error_metrics_deep_ar_plus, \"DeepAR\")\n",
    "prophet_metrics = extract_summary_metrics(error_metrics_prophet, \"Prophet\")\n",
    "auto_metrics = extract_summary_metrics(error_metrics_auto, \"Auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([deep_ar_metrics, prophet_metrics, auto_metrics]) \\\n",
    "    .pivot(index='Quantile', columns='Predictor', values='LossValue').plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_name_prefix_deep_ar = f\"{dataset_group_name}_deeparp\"\n",
    "forecast_name_deep_ar = f\"{forecast_name_prefix_deep_ar}_{datetime.now().strftime(date_format)}\"\n",
    "print(f\"Forecast Name = {forecast_name_deep_ar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_forecasts_deep_ar = util.get_forecasts(forecast_name_prefix_deep_ar, forecast, False)\n",
    "create_new_forecast = True\n",
    "if len(existing_forecasts_deep_ar) > 0:\n",
    "    print(f\"Forecasts exist with the latest one from {existing_forecasts_deep_ar[0]['CreationTime'].strftime(ui_date_format)}.\")\n",
    "    if not input(\"Create new forecast (y/N)? \").lower() == \"y\":\n",
    "        forecast_arn_deep_ar = existing_forecasts_deep_ar[0][\"ForecastArn\"]\n",
    "        create_new_forecast = False\n",
    "\n",
    "if create_new_forecast:\n",
    "    create_forecast_response_deep_ar = forecast.create_forecast(ForecastName=forecast_name_deep_ar,\n",
    "                                                                PredictorArn=predictor_arn_deep_ar,\n",
    "                                                                Tags=tags\n",
    "                                                               )\n",
    "    forecast_arn_deep_ar = create_forecast_response_deep_ar['ForecastArn']\n",
    "    status = util.wait(lambda: forecast.describe_forecast(ForecastArn=forecast_arn_deep_ar))\n",
    "    assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_response_deep = forecastquery.query_forecast(\n",
    "    ForecastArn=forecast_arn_deep_ar,\n",
    "    Filters={\"item_id\": item_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_response_deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact = util.load_exact_sol(s3_target_data_key, item_id, target_col_name=target_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecasts(fcsts, exact, freq = '1D', forecastHorizon=30, time_back = 30, target_col_name='target', reverse=False):\n",
    "    p10 = pd.DataFrame(fcsts['Forecast']['Predictions']['p10'])\n",
    "    p50 = pd.DataFrame(fcsts['Forecast']['Predictions']['p50'])\n",
    "    p90 = pd.DataFrame(fcsts['Forecast']['Predictions']['p90'])\n",
    "    pred_int = p50['Timestamp'].apply(lambda x: pd.Timestamp(x))\n",
    "    fcst_start_date = pred_int.iloc[0]\n",
    "    fcst_end_date = pred_int.iloc[-1]\n",
    "    time_int = exact['timestamp'].apply(lambda x: pd.Timestamp(x))\n",
    "    if reverse:\n",
    "        plt.plot(time_int.head(time_back),\n",
    "                 exact[target_col_name].head(time_back).values,\n",
    "                 color = 'r')\n",
    "    else:\n",
    "        plt.plot(time_int[-time_back:],\n",
    "                 exact[target_col_name].values[-time_back:],\n",
    "                 color = 'r')\n",
    "    plt.plot(pred_int, p50['Value'].values, color = 'k')\n",
    "    plt.fill_between(pred_int, \n",
    "                     p10['Value'].values,\n",
    "                     p90['Value'].values,\n",
    "                     color='b', alpha=0.3);\n",
    "    plt.axvline(x=pd.Timestamp(fcst_start_date), linewidth=1, color='g', ls='dashed')\n",
    "    plt.axvline(x=pd.Timestamp(fcst_end_date), linewidth=1, color='g', ls='dashed')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.legend(['Target', 'Forecast'], loc = 'lower left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecasts(forecast_response_deep,\n",
    "                    exact,\n",
    "                    freq=f'1{DATASET_FREQUENCY}',\n",
    "                    forecastHorizon=FORECAST_LENGTH,\n",
    "                    time_back=30,\n",
    "                    target_col_name=target_column_name,\n",
    "                    reverse=True\n",
    "                   )\n",
    "plt.title(\"DeepAR Forecast\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "# p=figure(min_width=500, height=500)\n",
    "\n",
    "exact.plot_bokeh.line(x='timestamp', y=[target_column_name], width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util.delete_dataset_group(dataset_group_arn, forecast)\n",
    "util.delete_dataset_group(\"arn:aws:forecast:us-east-1:275279264324:dataset-group/mac_training_rivn_forecast_1\", forecast)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
