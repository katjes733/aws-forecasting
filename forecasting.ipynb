{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "* Following tasks must be performed each time the notebook instance is started.\n",
    "* This is **NOT** required when the Kernel is restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Installation of `jupyter_bokeh` may take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conda install jupyter_bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Start here when you restarted your Kernel **ONLY**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"da0736c4-e599-4972-a241-d6ab5c5d6647\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"da0736c4-e599-4972-a241-d6ab5c5d6647\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.0.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"da0736c4-e599-4972-a241-d6ab5c5d6647\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"da0736c4-e599-4972-a241-d6ab5c5d6647\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"da0736c4-e599-4972-a241-d6ab5c5d6647\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# importing forecast notebook utility from notebooks/common directory\n",
    "sys.path.insert(0, os.path.abspath(\"./common/\"))\n",
    "import util\n",
    "import util.fcst_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (15.0, 5.0)\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import show\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resource tag values\n",
    "owner = \"martin.macecek@rearc.io\"\n",
    "type = \"Internal\"\n",
    "usage = \"Playground\"\n",
    "\n",
    "# Custom variables\n",
    "prefix = \"mac-training\"\n",
    "role_name = f\"{prefix}-forecasting\"\n",
    "bucket_name = f\"{prefix}-bucket-275279264324-us-east-1\"\n",
    "path = \"forecasting/input\"\n",
    "data_key = f\"{path}/RIVN.csv\"\n",
    "future_data_key = f\"{path}/RIVN_20240513.csv\"\n",
    "prepared_data_key_prefix = \"forecasting/prepared/rivn\"\n",
    "item_id = \"RIVN\"\n",
    "target_column_name = \"close\"\n",
    "\n",
    "# Setup more variables\n",
    "s3_target_data_key = f\"s3://{bucket_name}/{prepared_data_key_prefix}.csv\"\n",
    "s3_related_data_key = f\"s3://{bucket_name}/{prepared_data_key_prefix}_rts.csv\"\n",
    "date_format = '%Y%m%d_%H%M%S'\n",
    "ui_date_format = '%a, %d %b %Y %H:%M:%S %Z'\n",
    "\n",
    "# Tags for resource tagging\n",
    "tags = [{'Key': 'Owner', 'Value': owner},\n",
    "        {'Key': 'Type', 'Value': type},\n",
    "        {'Key': 'Usage', 'Value': usage}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3: API connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account: 275279264324, Region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "print(f\"Account: {account_id}, Region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect API sessions\n",
    "session = boto3.Session(region_name=region) \n",
    "s3 = session.client(service_name='s3')\n",
    "forecast = session.client(service_name='forecast')\n",
    "forecastquery = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: AWS resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The role mac-training-forecasting exists, skipping creation...\n",
      "Done.\n",
      "Success! Role 'mac-training-forecasting' ready for use.\n"
     ]
    }
   ],
   "source": [
    "# Create or retrieve the role to provide to Amazon Forecast.\n",
    "role_arn = util.get_or_create_iam_role(role_name=role_name)\n",
    "\n",
    "# echo user inputs without account\n",
    "print(f\"Success! Role '{role_arn.split('/')[1]}' ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Bucket 'mac-training-bucket-275279264324-us-east-1' ready for use.\n"
     ]
    }
   ],
   "source": [
    "util.get_or_create_bucket(bucket_name, region=region)\n",
    "print(f\"Success! Bucket '{bucket_name}' ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/fsspec/registry.py:275: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-13</td>\n",
       "      <td>10.90</td>\n",
       "      <td>10.04</td>\n",
       "      <td>11.19</td>\n",
       "      <td>10.04</td>\n",
       "      <td>RIVN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-12</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.40</td>\n",
       "      <td>9.84</td>\n",
       "      <td>RIVN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-11</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.40</td>\n",
       "      <td>9.84</td>\n",
       "      <td>RIVN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-05-10</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.40</td>\n",
       "      <td>9.84</td>\n",
       "      <td>RIVN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-09</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.25</td>\n",
       "      <td>10.47</td>\n",
       "      <td>10.04</td>\n",
       "      <td>RIVN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    datetime  close   open   high    low item_id\n",
       "0 2024-05-13  10.90  10.04  11.19  10.04    RIVN\n",
       "1 2024-05-12   9.99  10.20  10.40   9.84    RIVN\n",
       "2 2024-05-11   9.99  10.20  10.40   9.84    RIVN\n",
       "3 2024-05-10   9.99  10.20  10.40   9.84    RIVN\n",
       "4 2024-05-09  10.22  10.25  10.47  10.04    RIVN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df = util.prepare_data(bucket_name, data_key, \"%m/%d/%Y\", target_column_name, item_id, fill_missing_values=True)\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stock_df.plot(x='datetime', y=[target_column_name, 'open', 'high', 'low'], figsize=(15, 8))\n",
    "plt.xlabel('Date Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(bucket_name, data_key, date_format, target_column_name, item_id, fill_missing_values=False, minimal=False, start_date=None, end_date=None):\n",
    "    df = pd.read_csv(f\"s3://{bucket_name}/{data_key}\", dtype=object)\n",
    "    df.drop([\"Vol.\", \"Change %\"], axis=1, inplace=True)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=date_format).dt.date\n",
    "    df[[\"Price\", \"Open\", \"High\", \"Low\"]] = df[[\"Price\", \"Open\", \"High\", \"Low\"]].astype(float)\n",
    "    df.rename(columns={'Date': 'datetime', 'Price': target_column_name, 'Open': 'open', 'High': 'high', 'Low': 'low'}, inplace=True)\n",
    "    df[\"item_id\"] = item_id\n",
    "    \n",
    "    if start_date and end_date:\n",
    "        mask = (df['datetime'] >= start_date.to_pydatetime().date()) & (df['datetime'] <= end_date.to_pydatetime().date())\n",
    "        df = df.loc[mask]\n",
    "    elif start_date and not end_date:\n",
    "        mask = (df['datetime'] >= start_date.to_pydatetime().date())\n",
    "        df = df.loc[mask]\n",
    "    elif not start_date and end_date:\n",
    "        mask = (df['datetime'] <= end_date.to_pydatetime().date())\n",
    "        df = df.loc[mask]\n",
    "\n",
    "    if fill_missing_values:\n",
    "        new_index = pd.Index(pd.date_range(df.datetime.min(), df.datetime.max()), name=\"datetime\")\n",
    "        df.set_index(\"datetime\").reindex(new_index)\n",
    "        df = df.set_index(\"datetime\").reindex(new_index).reset_index().ffill()\n",
    "        df = df.sort_values(by=['datetime'], ascending=False, ignore_index=True)\n",
    "\n",
    "    if minimal:\n",
    "        df.drop([\"open\", \"high\", \"low\"], axis=1, inplace=True)\n",
    "        df.rename(columns={'datetime': 'timestamp'}, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_excluded(obj: str, exclude: list[str]=[]):\n",
    "    if not exclude:\n",
    "        return False\n",
    "    \n",
    "    for one_exclude in exclude:\n",
    "        if one_exclude in obj:\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_related_data(s3_client, bucket: str, prefix: str, exclude: list[str]=[], start_date=None, end_date=None):\n",
    "    object_prefix = prefix if prefix.endswith('/') else f\"{prefix}/\"\n",
    "    \n",
    "    args = {\n",
    "        \"Bucket\": bucket,\n",
    "        \"Prefix\": object_prefix\n",
    "    }\n",
    "    related_data_csv_objects = {}\n",
    "    while True:\n",
    "        response = s3_client.list_objects_v2(**args)\n",
    "        list_related_data_csv_objects = list(filter(lambda obj: obj != object_prefix and not is_excluded(obj, exclude), list(map(lambda obj: obj[\"Key\"], response[\"Contents\"]))))\n",
    "        related_data_csv_objects = { Path(obj).stem.lower() : obj for obj in list_related_data_csv_objects}\n",
    "        if \"NextContinuationToken\" in response:\n",
    "            args[\"ContinuationToken\"] = response[\"NextContinuationToken\"]\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    related_stock_dfs = []\n",
    "    for key in related_data_csv_objects:\n",
    "        related_stock_dfs.append(\n",
    "            prepare_data(\n",
    "                bucket,\n",
    "                related_data_csv_objects[key],\n",
    "                \"%m/%d/%Y\",\n",
    "                target_column_name,\n",
    "                key,\n",
    "                fill_missing_values=True,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return related_stock_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[      datetime   close    open    high     low item_id\n",
       " 0   2024-05-13  186.57  188.00  188.18  185.36    amzn\n",
       " 1   2024-05-12  187.48  189.16  189.87  186.93    amzn\n",
       " 2   2024-05-11  187.48  189.16  189.87  186.93    amzn\n",
       " 3   2024-05-10  187.48  189.16  189.87  186.93    amzn\n",
       " 4   2024-05-09  189.50  188.88  191.68  187.52    amzn\n",
       " ..         ...     ...     ...     ...     ...     ...\n",
       " 910 2021-11-15  177.28  176.85  179.69  176.29    amzn\n",
       " 911 2021-11-14  176.26  174.25  177.04  172.35    amzn\n",
       " 912 2021-11-13  176.26  174.25  177.04  172.35    amzn\n",
       " 913 2021-11-12  176.26  174.25  177.04  172.35    amzn\n",
       " 914 2021-11-11  173.62  175.65  177.16  173.37    amzn\n",
       " \n",
       " [915 rows x 6 columns],\n",
       "       datetime  close   open   high    low item_id\n",
       " 0   2024-05-13  12.32  12.09  12.45  12.09    ford\n",
       " 1   2024-05-12  11.99  12.14  12.17  11.92    ford\n",
       " 2   2024-05-11  11.99  12.14  12.17  11.92    ford\n",
       " 3   2024-05-10  11.99  12.14  12.17  11.92    ford\n",
       " 4   2024-05-09  12.14  12.12  12.21  12.06    ford\n",
       " ..         ...    ...    ...    ...    ...     ...\n",
       " 910 2021-11-15  18.90  18.52  18.99  18.22    ford\n",
       " 911 2021-11-14  18.56  18.45  18.67  18.28    ford\n",
       " 912 2021-11-13  18.56  18.45  18.67  18.28    ford\n",
       " 913 2021-11-12  18.56  18.45  18.67  18.28    ford\n",
       " 914 2021-11-11  18.61  18.69  18.88  18.34    ford\n",
       " \n",
       " [915 rows x 6 columns],\n",
       "       datetime  close   open   high    low item_id\n",
       " 0   2024-05-13  45.16  45.58  45.97  45.08      gm\n",
       " 1   2024-05-12  45.20  45.59  45.60  45.05      gm\n",
       " 2   2024-05-11  45.20  45.59  45.60  45.05      gm\n",
       " 3   2024-05-10  45.20  45.59  45.60  45.05      gm\n",
       " 4   2024-05-09  45.37  44.93  45.65  44.81      gm\n",
       " ..         ...    ...    ...    ...    ...     ...\n",
       " 910 2021-11-15  62.97  63.65  63.73  62.63      gm\n",
       " 911 2021-11-14  63.40  61.59  64.03  61.28      gm\n",
       " 912 2021-11-13  63.40  61.59  64.03  61.28      gm\n",
       " 913 2021-11-12  63.40  61.59  64.03  61.28      gm\n",
       " 914 2021-11-11  61.82  59.82  62.15  59.31      gm\n",
       " \n",
       " [915 rows x 6 columns],\n",
       "       datetime  close   open    high    low item_id\n",
       " 0   2024-05-13   2.81   2.69   3.063   2.68    lcid\n",
       " 1   2024-05-12   2.66   2.71   2.745   2.60    lcid\n",
       " 2   2024-05-11   2.66   2.71   2.745   2.60    lcid\n",
       " 3   2024-05-10   2.66   2.71   2.745   2.60    lcid\n",
       " 4   2024-05-09   2.70   2.70   2.740   2.67    lcid\n",
       " ..         ...    ...    ...     ...    ...     ...\n",
       " 910 2021-11-15  44.88  44.60  46.140  43.41    lcid\n",
       " 911 2021-11-14  43.93  44.00  45.190  42.74    lcid\n",
       " 912 2021-11-13  43.93  44.00  45.190  42.74    lcid\n",
       " 913 2021-11-12  43.93  44.00  45.190  42.74    lcid\n",
       " 914 2021-11-11  44.98  42.75  45.750  41.74    lcid\n",
       " \n",
       " [915 rows x 6 columns],\n",
       "       datetime    close    open     high      low item_id\n",
       " 0   2024-05-13   0.5499   0.540   0.5770   0.5400    nkla\n",
       " 1   2024-05-12   0.5370   0.580   0.5800   0.5301    nkla\n",
       " 2   2024-05-11   0.5370   0.580   0.5800   0.5301    nkla\n",
       " 3   2024-05-10   0.5370   0.580   0.5800   0.5301    nkla\n",
       " 4   2024-05-09   0.5760   0.585   0.5932   0.5740    nkla\n",
       " ..         ...      ...     ...      ...      ...     ...\n",
       " 910 2021-11-15  13.7300  14.020  14.1300  13.4000    nkla\n",
       " 911 2021-11-14  13.9800  14.350  14.4000  13.7200    nkla\n",
       " 912 2021-11-13  13.9800  14.350  14.4000  13.7200    nkla\n",
       " 913 2021-11-12  13.9800  14.350  14.4000  13.7200    nkla\n",
       " 914 2021-11-11  14.3600  14.110  14.8000  13.6000    nkla\n",
       " \n",
       " [915 rows x 6 columns],\n",
       "       datetime   close    open    high     low item_id\n",
       " 0   2024-05-13  171.89  170.00  175.39  169.09    tsla\n",
       " 1   2024-05-12  168.47  173.05  172.96  167.75    tsla\n",
       " 2   2024-05-11  168.47  173.05  172.96  167.75    tsla\n",
       " 3   2024-05-10  168.47  173.05  172.96  167.75    tsla\n",
       " 4   2024-05-09  171.97  175.01  175.43  171.40    tsla\n",
       " ..         ...     ...     ...     ...     ...     ...\n",
       " 910 2021-11-15  337.80  339.21  343.99  326.20    tsla\n",
       " 911 2021-11-14  344.47  349.17  351.50  339.73    tsla\n",
       " 912 2021-11-13  344.47  349.17  351.50  339.73    tsla\n",
       " 913 2021-11-12  344.47  349.17  351.50  339.73    tsla\n",
       " 914 2021-11-11  354.50  367.59  368.32  351.56    tsla\n",
       " \n",
       " [915 rows x 6 columns]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_related_data(s3, bucket_name, path, [data_key, future_data_key], start_date=stock_df.datetime.min(), end_date=stock_df.datetime.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Prepare and Save the Target Time Series\n",
    "\n",
    "In this exmple, we are only using the supplemental fields `open`, `high` and `low` for forecasting.\n",
    "Later we may explore with supplemental values from other stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Forecast length in days (Units is defined below)\n",
    "FORECAST_LENGTH = 30\n",
    "\n",
    "# What is your forecast time unit granularity?\n",
    "# Choices are: ^Y|M|W|D|h|30min|15min|10min|5min|1min$ \n",
    "DATASET_FREQUENCY = \"D\"\n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd\"\n",
    "# delimiter = ','\n",
    "\n",
    "# What name do you want to give this project?\n",
    "# We will use this same name for your Forecast Dataset Group name.\n",
    "PROJECT = 'rivn-forecast'\n",
    "DATA_VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_df = stock_df[['item_id', 'datetime', target_column_name]][:-FORECAST_LENGTH]\n",
    "target_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rts_df = stock_df[['item_id', 'datetime', 'open', 'high', 'low']]\n",
    "rts_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{len(target_df)} + {FORECAST_LENGTH} = {len(rts_df)}\")\n",
    "assert len(target_df) + FORECAST_LENGTH == len(rts_df), \"length doesn't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_df.to_csv(s3_target_data_key, index= False, header = False)\n",
    "rts_df.to_csv(s3_related_data_key, index= False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create the Dataset Group and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_group_name = f\"{prefix}_{PROJECT}_{DATA_VERSION}\".replace(\"-\", \"_\")\n",
    "print(f\"Dataset Group Name = {dataset_group_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_arns = []\n",
    "try:\n",
    "    create_dataset_group_response = \\\n",
    "        forecast.create_dataset_group(Domain=\"RETAIL\",\n",
    "                                      DatasetGroupName=dataset_group_name,\n",
    "                                      DatasetArns=dataset_arns,\n",
    "                                      Tags=tags\n",
    "                                     )\n",
    "    dataset_group_arn = create_dataset_group_response['DatasetGroupArn']\n",
    "    status = util.wait(lambda: forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn))\n",
    "    assert status\n",
    "except forecast.exceptions.ResourceAlreadyExistsException:\n",
    "    dataset_group_arn = f\"arn:aws:forecast:{region}:{account_id}:dataset-group/{dataset_group_name}\"\n",
    "    print(f\"Dataset group {dataset_group_arn} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the schema of your dataset here. Make sure the order of columns matches the raw data files.\n",
    "tts_schema = {\n",
    "   \"Attributes\": [\n",
    "      {\n",
    "         \"AttributeName\": \"item_id\",\n",
    "         \"AttributeType\": \"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\": \"timestamp\",\n",
    "         \"AttributeType\": \"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\": \"demand\",\n",
    "         \"AttributeType\": \"float\"\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tts_dataset_name = f\"{dataset_group_name}_tts\"\n",
    "print(tts_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    create_dataset_tts_response = \\\n",
    "        forecast.create_dataset(Domain=\"RETAIL\",\n",
    "                                DatasetType='TARGET_TIME_SERIES',\n",
    "                                DatasetName=tts_dataset_name,\n",
    "                                DataFrequency=DATASET_FREQUENCY,\n",
    "                                Schema=tts_schema,\n",
    "                                Tags=tags\n",
    "                               )\n",
    "    tts_dataset_arn = create_dataset_tts_response['DatasetArn']\n",
    "    status = util.wait(lambda: forecast.describe_dataset(DatasetArn=tts_dataset_arn))\n",
    "    assert status\n",
    "except forecast.exceptions.ResourceAlreadyExistsException:\n",
    "    tts_dataset_arn = f\"arn:aws:forecast:{region}:{account_id}:dataset/{tts_dataset_name}\"\n",
    "    print(f\"Target dataset {tts_dataset_arn} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the schema of your dataset here. Make sure the order of columns matches the raw data files.\n",
    "rts_schema = {\n",
    "   \"Attributes\": [\n",
    "      {\n",
    "         \"AttributeName\": \"item_id\",\n",
    "         \"AttributeType\": \"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\": \"timestamp\",\n",
    "         \"AttributeType\": \"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\": \"open_price\",\n",
    "         \"AttributeType\": \"float\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\": \"high_price\",\n",
    "         \"AttributeType\": \"float\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\": \"low_price\",\n",
    "         \"AttributeType\": \"float\"\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rts_dataset_name = f\"{dataset_group_name}_rts\"\n",
    "print(rts_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    create_dataset_rts_response = \\\n",
    "        forecast.create_dataset(Domain=\"RETAIL\",\n",
    "                                DatasetType='RELATED_TIME_SERIES',\n",
    "                                DatasetName=rts_dataset_name,\n",
    "                                DataFrequency=DATASET_FREQUENCY,\n",
    "                                Schema=rts_schema,\n",
    "                                Tags=tags\n",
    "                               )\n",
    "    rts_dataset_arn = create_dataset_rts_response['DatasetArn']\n",
    "    status = util.wait(lambda: forecast.describe_dataset(DatasetArn=rts_dataset_arn))\n",
    "    assert status\n",
    "except forecast.exceptions.ResourceAlreadyExistsException:\n",
    "    rts_dataset_arn = f\"arn:aws:forecast:{region}:{account_id}:dataset/{rts_dataset_name}\"\n",
    "    print(f\"Related dataset {rts_dataset_arn} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_arns = []\n",
    "dataset_arns.append(tts_dataset_arn)\n",
    "dataset_arns.append(rts_dataset_arn)\n",
    "update_dataset_response = forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=dataset_arns)\n",
    "status = util.wait(lambda: forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Import data from S3 to Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Depending on the data size, the import can take 10 mins or more to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import_tts_dataset_response = True\n",
    "if len(util.get_dataset_import_jobs(tts_dataset_arn, forecast)) > 0:\n",
    "    print(\"Target dataset has already imported data.\")\n",
    "    import_tts_dataset_response = True if input(\"Re-import (y/N)? \").lower() == \"y\" else False\n",
    "\n",
    "if import_tts_dataset_response:\n",
    "    tts_dataset_import_job_response = \\\n",
    "        forecast.create_dataset_import_job(DatasetImportJobName=f\"tts_job_{datetime.now().strftime(date_format)}\",\n",
    "                                           DatasetArn=tts_dataset_arn,\n",
    "                                           DataSource= {\n",
    "                                             \"S3Config\" : {\n",
    "                                                 \"Path\": s3_target_data_key,\n",
    "                                                 \"RoleArn\": role_arn\n",
    "                                             } \n",
    "                                           },\n",
    "                                           TimestampFormat=TIMESTAMP_FORMAT,\n",
    "                                           Tags=tags\n",
    "                                          )\n",
    "    tts_dataset_import_job_arn=tts_dataset_import_job_response['DatasetImportJobArn']\n",
    "    status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=tts_dataset_import_job_arn))\n",
    "    assert status\n",
    "    print(\"Target data imported.\")\n",
    "else:\n",
    "    print(\"Target data re-import skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Depending on the data size, the import can take 10 mins or more to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import_rts_dataset_response = True\n",
    "if len(util.get_dataset_import_jobs(rts_dataset_arn, forecast)) > 0:\n",
    "    print(\"Related dataset has already imported data.\")\n",
    "    import_rts_dataset_response = True if input(\"Re-import (y/N)? \").lower() == \"y\" else False\n",
    "\n",
    "if import_rts_dataset_response:\n",
    "    rts_dataset_import_job_response = \\\n",
    "        forecast.create_dataset_import_job(DatasetImportJobName=f\"rts_job_{datetime.now().strftime(date_format)}\",\n",
    "                                           DatasetArn=rts_dataset_arn,\n",
    "                                           DataSource= {\n",
    "                                             \"S3Config\" : {\n",
    "                                                 \"Path\": s3_related_data_key,\n",
    "                                                 \"RoleArn\": role_arn\n",
    "                                             } \n",
    "                                           },\n",
    "                                           TimestampFormat=TIMESTAMP_FORMAT,\n",
    "                                           Tags=tags\n",
    "                                          )\n",
    "    rts_dataset_import_job_arn = rts_dataset_import_job_response[\"DatasetImportJobArn\"]\n",
    "    status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=rts_dataset_import_job_arn))\n",
    "    assert status\n",
    "    print(\"Related data imported.\")\n",
    "else:\n",
    "    print(\"Related data re-import skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algorithm_arn = 'arn:aws:forecast:::algorithm/'\n",
    "algorithm = 'Deep_AR_Plus'\n",
    "algorithm_arn_deep_ar_plus = algorithm_arn + algorithm\n",
    "predictor_name_deep_ar = f\"{dataset_group_name}_{algorithm.lower()}\"\n",
    "print(f\"Predictor Name = {predictor_name_deep_ar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Training a forecast model can take several hours to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrain_predictor_deep_ar = False\n",
    "existing_predictor_deep_ar = util.get_predictor(predictor_name_deep_ar, forecast)\n",
    "\n",
    "if existing_predictor_deep_ar:\n",
    "    predictor_arn_deep_ar = existing_predictor_deep_ar['PredictorArn']\n",
    "    print(f\"DeepAR+ Predictor {predictor_arn_deep_ar} already exists.\")\n",
    "    retrain_predictor_deep_ar = True if input(\"Retrain model (y/N)? \").lower() == \"y\" else False\n",
    "\n",
    "if existing_predictor_deep_ar and retrain_predictor_deep_ar:\n",
    "    util.delete_forecasts_by_predictor(predictor_arn_deep_ar, forecast)\n",
    "    print(f\"Deleting DeepAR+ Predictor {predictor_arn_deep_ar}...\")\n",
    "    util.wait_till_delete(lambda: forecast.delete_predictor(PredictorArn=predictor_arn_deep_ar))\n",
    "elif existing_predictor_deep_ar and not retrain_predictor_deep_ar:\n",
    "    print(f\"Keeping existing DeepAR+ Predictor {predictor_arn_deep_ar}.\")\n",
    "\n",
    "if not existing_predictor_deep_ar or retrain_predictor_deep_ar:\n",
    "    create_predictor_deep_ar_response = \\\n",
    "        forecast.create_predictor(PredictorName=predictor_name_deep_ar,\n",
    "                                  AlgorithmArn=algorithm_arn_deep_ar_plus,\n",
    "                                  ForecastHorizon=FORECAST_LENGTH,\n",
    "                                  PerformAutoML=False,\n",
    "                                  PerformHPO=False,\n",
    "                                  InputDataConfig={\n",
    "                                      \"DatasetGroupArn\": dataset_group_arn,\n",
    "                                      \"SupplementaryFeatures\": [\n",
    "                                          {\"Name\": \"holiday\",\n",
    "                                           \"Value\": \"US\"}],\n",
    "                                  },\n",
    "                                  FeaturizationConfig={\"ForecastFrequency\": DATASET_FREQUENCY},\n",
    "                                  Tags=tags\n",
    "                                 )\n",
    "    predictor_arn_deep_ar = create_predictor_deep_ar_response['PredictorArn']\n",
    "    print(f\"Creating DeepAR+ Predictor {predictor_arn_deep_ar}...\")\n",
    "    status = util.wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_deep_ar))\n",
    "    assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn = 'arn:aws:forecast:::algorithm/'\n",
    "algorithm = 'Prophet'\n",
    "algorithm_arn_prophet = algorithm_arn + algorithm\n",
    "predictor_name_prophet = f\"{dataset_group_name}_{algorithm.lower()}\"\n",
    "print(f\"Predictor Name = {predictor_name_prophet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Training a forecast model can take several hours to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_predictor_prophet = False\n",
    "existing_predictor_prophet = util.get_predictor(predictor_name_prophet, forecast)\n",
    "\n",
    "if existing_predictor_prophet:\n",
    "    predictor_arn_prophet = existing_predictor_prophet['PredictorArn']\n",
    "    print(f\"Prophet Predictor {predictor_arn_prophet} already exists.\")\n",
    "    retrain_predictor_prophet = True if input(\"Retrain model (y/N)? \").lower() == \"y\" else False\n",
    "\n",
    "if existing_predictor_prophet and retrain_predictor_prophet:\n",
    "    print(f\"Deleting Prophet Predictor {predictor_arn_prophet}...\")\n",
    "    util.wait_till_delete(lambda: forecast.delete_predictor(PredictorArn=predictor_arn_prophet))\n",
    "elif existing_predictor_prophet and not retrain_predictor_prophet:\n",
    "    print(f\"Keeping existing Prophet Predictor {predictor_arn_prophet}.\")\n",
    "\n",
    "if not existing_predictor_prophet or retrain_predictor_prophet:\n",
    "    create_predictor_response = \\\n",
    "        forecast.create_predictor(PredictorName=predictor_name_prophet,\n",
    "                                  AlgorithmArn=algorithm_arn_prophet,\n",
    "                                  ForecastHorizon=FORECAST_LENGTH,\n",
    "                                  PerformAutoML=False,\n",
    "                                  PerformHPO=False,\n",
    "                                  InputDataConfig= {\n",
    "                                      \"DatasetGroupArn\": dataset_group_arn,\n",
    "                                      \"SupplementaryFeatures\": [\n",
    "                                          {\"Name\": \"holiday\",\n",
    "                                           \"Value\": \"US\"}],\n",
    "                                  },\n",
    "                                  FeaturizationConfig={\"ForecastFrequency\": DATASET_FREQUENCY},\n",
    "                                  Tags=tags\n",
    "                                 )\n",
    "    predictor_arn_prophet = create_predictor_response['PredictorArn']\n",
    "    print(f\"Creating Prophet Predictor {predictor_arn_prophet}...\")\n",
    "    status = util.wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_prophet))\n",
    "    assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_name_auto = f\"{dataset_group_name}_auto\"\n",
    "print(f\"Predictor Name = {predictor_name_auto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Training a forecast model can take several hours to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "existing_predictor_auto = util.get_predictor(predictor_name_auto, forecast)\n",
    "if existing_predictor_auto:\n",
    "    predictor_arn_auto = existing_predictor_auto['PredictorArn']\n",
    "    print(f\"Auto Predictor {predictor_arn_auto} already exists.\")\n",
    "    if input(\"Retrain model (y/N)? \").lower() == \"y\":\n",
    "        args = {\n",
    "            \"PredictorName\": f\"{predictor_name_auto}_retrain_{datetime.now().strftime(date_format)}\",\n",
    "            \"ReferencePredictorArn\": predictor_arn_auto\n",
    "        }\n",
    "    else:\n",
    "        args = {}\n",
    "else:\n",
    "    args = {\n",
    "        \"PredictorName\": predictor_name_auto,\n",
    "        \"ForecastHorizon\": FORECAST_LENGTH,\n",
    "        \"ForecastFrequency\": DATASET_FREQUENCY,\n",
    "        \"DataConfig\": {\n",
    "            \"DatasetGroupArn\": dataset_group_arn,\n",
    "            \"AdditionalDatasets\": [\n",
    "                {\n",
    "                    \"Name\": \"holiday\",\n",
    "                    \"Configuration\": {\n",
    "                        \"CountryCode\": [\"US\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"Tags\": tags\n",
    "    }\n",
    "\n",
    "if args:\n",
    "    create_predictor_auto_response = forecast.create_auto_predictor(**args)\n",
    "    predictor_arn_auto = create_predictor_auto_response['PredictorArn']\n",
    "    print(f\"{'Retraining existing' if existing_predictor_auto else 'Creating new'} Auto Predictor {predictor_arn_auto}...\")\n",
    "    status = util.wait(lambda: forecast.describe_auto_predictor(PredictorArn=predictor_arn_auto))\n",
    "    assert status\n",
    "else:\n",
    "    print(f\"Keeping existing Auto Predictor {predictor_arn_auto}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Predictor Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_deep_ar_plus = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_deep_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_prophet = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_auto = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_metrics(metric_response, predictor_name):\n",
    "    df = pd.DataFrame(metric_response['PredictorEvaluationResults']\n",
    "                 [0]['TestWindows'][0]['Metrics']['WeightedQuantileLosses'])\n",
    "    df['Predictor'] = predictor_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_ar_metrics = extract_summary_metrics(error_metrics_deep_ar_plus, \"DeepAR\")\n",
    "prophet_metrics = extract_summary_metrics(error_metrics_prophet, \"Prophet\")\n",
    "auto_metrics = extract_summary_metrics(error_metrics_auto, \"Auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([deep_ar_metrics, prophet_metrics, auto_metrics]) \\\n",
    "    .pivot(index='Quantile', columns='Predictor', values='LossValue').plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Creating a forecast can take up to an hour to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_arn_deep_ar = util.create_forecast(\n",
    "    *util.create_forecast_name(dataset_group_name, \n",
    "                               'deep_ar',\n",
    "                               date_format=date_format\n",
    "                              ),\n",
    "    predictor_arn_deep_ar,\n",
    "    forecast,\n",
    "    tags,\n",
    "    ui_date_format=ui_date_format,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Creating a forecast can take up to an hour to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_arn_prophet = util.create_forecast(\n",
    "    *util.create_forecast_name(dataset_group_name, \n",
    "                               'prophet',\n",
    "                               date_format=date_format\n",
    "                              ),\n",
    "    predictor_arn_prophet,\n",
    "    forecast,\n",
    "    tags,\n",
    "    ui_date_format=ui_date_format,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Creating a forecast can take up to an hour to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_arn_auto = util.create_forecast(\n",
    "    *util.create_forecast_name(dataset_group_name, \n",
    "                               'auto',\n",
    "                               date_format=date_format\n",
    "                              ),\n",
    "    predictor_arn_auto,\n",
    "    forecast,\n",
    "    tags,\n",
    "    ui_date_format=ui_date_format,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exact = util.load_exact_sol(s3_target_data_key, item_id, target_col_name=target_column_name)\n",
    "exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "future_df = prepare_data(bucket_name, future_data_key, \"%m/%d/%Y\", target_column_name, fill_missing_values=True, minimal=True)\n",
    "future_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_response_deep_ar = forecastquery.query_forecast(\n",
    "    ForecastArn=forecast_arn_deep_ar,\n",
    "    Filters={\"item_id\": item_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_response_deep_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot using default plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "util.plot_forecasts(\n",
    "    forecast_response_deep_ar,\n",
    "    exact,\n",
    "    freq=f'1{DATASET_FREQUENCY}',\n",
    "    forecastHorizon=FORECAST_LENGTH,\n",
    "    time_back=30,\n",
    "    future=future_df,\n",
    "    target_col_name=target_column_name,\n",
    "    reverse=True\n",
    ")\n",
    "plt.title(\"DeepAR Forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot using bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deep_ar_plot = util.plot_bokeh_forecasts(\n",
    "    forecast_response_deep_ar,\n",
    "    exact,\n",
    "    freq=f'1{DATASET_FREQUENCY}',\n",
    "    forecastHorizon=FORECAST_LENGTH,\n",
    "    time_back=30,\n",
    "    future=future_df,\n",
    "    target_col_name=target_column_name,\n",
    "    reverse=True\n",
    ")\n",
    "show(deep_ar_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_response_prophet = forecastquery.query_forecast(\n",
    "    ForecastArn=forecast_arn_prophet,\n",
    "    Filters={\"item_id\": item_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_response_prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot using default plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "util.plot_forecasts(\n",
    "    forecast_response_prophet,\n",
    "    exact,\n",
    "    freq=f'1{DATASET_FREQUENCY}',\n",
    "    forecastHorizon=FORECAST_LENGTH,\n",
    "    time_back=30,\n",
    "    future=future_df,\n",
    "    target_col_name=target_column_name,\n",
    "    reverse=True\n",
    ")\n",
    "plt.title(\"Prophet Forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot using bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prophet_plot = util.plot_bokeh_forecasts(\n",
    "    forecast_response_prophet,\n",
    "    exact,\n",
    "    freq=f'1{DATASET_FREQUENCY}',\n",
    "    forecastHorizon=FORECAST_LENGTH,\n",
    "    time_back=30,\n",
    "    future=future_df,\n",
    "    target_col_name=target_column_name,\n",
    "    reverse=True\n",
    ")\n",
    "show(prophet_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_response_auto = forecastquery.query_forecast(\n",
    "    ForecastArn=forecast_arn_auto,\n",
    "    Filters={\"item_id\": item_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_response_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot using default plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "util.plot_forecasts(\n",
    "    forecast_response_auto,\n",
    "    exact,\n",
    "    freq=f'1{DATASET_FREQUENCY}',\n",
    "    forecastHorizon=FORECAST_LENGTH,\n",
    "    time_back=30,\n",
    "    future=future_df,\n",
    "    target_col_name=target_column_name,\n",
    "    reverse=True\n",
    ")\n",
    "plt.title(\"Auto Forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot using bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auto_plot = util.plot_bokeh_forecasts(\n",
    "    forecast_response_auto,\n",
    "    exact,\n",
    "    freq=f'1{DATASET_FREQUENCY}',\n",
    "    forecastHorizon=FORECAST_LENGTH,\n",
    "    time_back=30,\n",
    "    future=future_df,\n",
    "    target_col_name=target_column_name,\n",
    "    reverse=True\n",
    ")\n",
    "show(auto_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util.delete_dataset_group(dataset_group_arn, forecast)\n",
    "util.delete_dataset_group(\"arn:aws:forecast:us-east-1:275279264324:dataset-group/mac_training_rivn_forecast_1\", forecast)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
