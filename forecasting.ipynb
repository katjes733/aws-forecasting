{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "* Following tasks must be performed each time the notebook instance is started.\n",
    "* This is **NOT** required when the Kernel is restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Installation of dependencies may take up to a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "%pip install pyod jupyter_bokeh\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"Time: {end - start:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Start here when you restarted your Kernel **ONLY**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# importing forecast notebook utility from notebooks/common directory\n",
    "sys.path.insert(0, os.path.abspath(\"./common/\"))\n",
    "import util\n",
    "import util.fcst_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (15.0, 5.0)\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import show\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resource tag values\n",
    "owner = \"martin.macecek@rearc.io\"\n",
    "type = \"Internal\"\n",
    "usage = \"Playground\"\n",
    "\n",
    "# Custom variables\n",
    "prefix = \"mac-training\"\n",
    "role_name = f\"{prefix}-forecasting\"\n",
    "bucket_name = f\"{prefix}-bucket-275279264324-us-east-1\"\n",
    "path = \"forecasting/input\"\n",
    "data_key = f\"{path}/RIVN.csv\"\n",
    "future_data_key = f\"{path}/RIVN_20240513.csv\"\n",
    "prepared_data_key_prefix = \"forecasting/prepared/rivn\"\n",
    "item_id = \"RIVN\"\n",
    "target_column_name = \"close\"\n",
    "domain=\"RETAIL\"\n",
    "\n",
    "# Setup more variables\n",
    "s3_target_data_key = f\"s3://{bucket_name}/{prepared_data_key_prefix}.csv\"\n",
    "s3_related_data_key = f\"s3://{bucket_name}/{prepared_data_key_prefix}_rts.csv\"\n",
    "date_format = '%Y%m%d_%H%M%S'\n",
    "ui_date_format = '%a, %d %b %Y %H:%M:%S %Z'\n",
    "\n",
    "# Tags for resource tagging\n",
    "tags = [{'Key': 'Owner', 'Value': owner},\n",
    "        {'Key': 'Type', 'Value': type},\n",
    "        {'Key': 'Usage', 'Value': usage}]\n",
    "\n",
    "# Forecast\n",
    "# Forecast length in days (Units is defined below)\n",
    "FORECAST_LENGTH = 30\n",
    "\n",
    "# What is your forecast time unit granularity?\n",
    "# Choices are: ^Y|M|W|D|h|30min|15min|10min|5min|1min$\n",
    "DATASET_FREQUENCY = \"D\"\n",
    "DATASET_TIMESTAMP_FORMAT = \"yyyy-MM-dd\"\n",
    "# delimiter = ','\n",
    "\n",
    "# What name do you want to give this project?\n",
    "# We will use this same name for your Forecast Dataset Group name.\n",
    "PROJECT = 'rivn-forecast'\n",
    "DATA_VERSION = 9\n",
    "\n",
    "# Whether or not to use other features of target dataet (e.g. high, low, open...)\n",
    "USE_OTHER_FEATURES = True\n",
    "# Whether or not to use bank days, which creates another features indicating a trading day or not\n",
    "USE_BANK_DAY = False\n",
    "# Whether or not to fill missing data (typically non-trading days (weekends and holidays))\n",
    "FILL_MISSING_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3: API connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "print(f\"Account: {account_id}, Region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect API sessions\n",
    "session = boto3.Session(region_name=region) \n",
    "s3 = session.client(service_name='s3')\n",
    "forecast = session.client(service_name='forecast')\n",
    "forecastquery = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: AWS resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create or retrieve the role to provide to Amazon Forecast.\n",
    "role_arn = util.get_or_create_iam_role(role_name=role_name)\n",
    "\n",
    "# echo user inputs without account\n",
    "print(f\"Success! Role '{role_arn.split('/')[1]}' ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "util.get_or_create_bucket(bucket_name, region=region)\n",
    "print(f\"Success! Bucket '{bucket_name}' ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stock_df = util.prepare_data(\n",
    "    bucket_name,\n",
    "    data_key,\n",
    "    \"%m/%d/%Y\",\n",
    "    target_column_name,\n",
    "    item_id,\n",
    "    fill_missing_values=FILL_MISSING_DATA,\n",
    "    use_bank_day=USE_BANK_DAY)\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stock_df.plot(x='timestamp', y=[target_column_name, 'open', 'high', 'low'], figsize=(15, 8))\n",
    "plt.xlabel('Date Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stock_df.timestamp.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "related_data=util.get_related_data(\n",
    "    s3_client=s3,\n",
    "    bucket=bucket_name,\n",
    "    prefix=path,\n",
    "    target_df=stock_df,\n",
    "    target_column_name=target_column_name,\n",
    "    item_id=item_id,\n",
    "    exclude=[data_key, future_data_key],\n",
    "    fill_missing_values=FILL_MISSING_DATA,\n",
    "    use_bank_day=USE_BANK_DAY,\n",
    "    extra_features=USE_OTHER_FEATURES,\n",
    "    start_date=stock_df.timestamp.min(),\n",
    "    end_date=stock_df.timestamp.max()\n",
    ")\n",
    "related_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Prepare and Save the Target Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_df = stock_df[[\"timestamp\", \"item_id\", target_column_name]]\n",
    "target_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rts_df = related_data\n",
    "rts_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{len(target_df)} = {len(rts_df)}\")\n",
    "assert len(target_df) == len(rts_df), \"length doesn't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_df.to_csv(s3_target_data_key, index= False, header = False)\n",
    "rts_df.to_csv(s3_related_data_key, index= False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create the Dataset Group and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_group_name = f\"{prefix}_{PROJECT}_{DATA_VERSION}\".replace(\"-\", \"_\")\n",
    "print(f\"Dataset Group Name = {dataset_group_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_arns = []\n",
    "try:\n",
    "    create_dataset_group_response = \\\n",
    "        forecast.create_dataset_group(Domain=domain,\n",
    "                                      DatasetGroupName=dataset_group_name,\n",
    "                                      DatasetArns=dataset_arns,\n",
    "                                      Tags=tags\n",
    "                                     )\n",
    "    dataset_group_arn = create_dataset_group_response['DatasetGroupArn']\n",
    "    status = util.wait(lambda: forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn))\n",
    "    assert status\n",
    "except forecast.exceptions.ResourceAlreadyExistsException:\n",
    "    dataset_group_arn = f\"arn:aws:forecast:{region}:{account_id}:dataset-group/{dataset_group_name}\"\n",
    "    print(f\"Dataset group {dataset_group_arn} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the schema of your dataset here. Make sure the order of columns matches the raw data files.\n",
    "tts_schema = {\n",
    "   \"Attributes\": util.get_schema_attributes(target_df, domain, target_column_name)\n",
    "}\n",
    "tts_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tts_dataset_name = f\"{dataset_group_name}_tts\"\n",
    "print(tts_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    create_dataset_tts_response = \\\n",
    "        forecast.create_dataset(Domain=domain,\n",
    "                                DatasetType='TARGET_TIME_SERIES',\n",
    "                                DatasetName=tts_dataset_name,\n",
    "                                DataFrequency=DATASET_FREQUENCY,\n",
    "                                Schema=tts_schema,\n",
    "                                Tags=tags\n",
    "                               )\n",
    "    tts_dataset_arn = create_dataset_tts_response['DatasetArn']\n",
    "    status = util.wait(lambda: forecast.describe_dataset(DatasetArn=tts_dataset_arn))\n",
    "    assert status\n",
    "except forecast.exceptions.ResourceAlreadyExistsException:\n",
    "    tts_dataset_arn = f\"arn:aws:forecast:{region}:{account_id}:dataset/{tts_dataset_name}\"\n",
    "    print(f\"Target dataset {tts_dataset_arn} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the schema of your dataset here. Make sure the order of columns matches the raw data files.\n",
    "rts_schema = {\n",
    "   \"Attributes\": util.get_schema_attributes(rts_df, domain, target_column_name)\n",
    "}\n",
    "rts_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rts_dataset_name = f\"{dataset_group_name}_rts\"\n",
    "print(rts_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    create_dataset_rts_response = \\\n",
    "        forecast.create_dataset(Domain=domain,\n",
    "                                DatasetType='RELATED_TIME_SERIES',\n",
    "                                DatasetName=rts_dataset_name,\n",
    "                                DataFrequency=DATASET_FREQUENCY,\n",
    "                                Schema=rts_schema,\n",
    "                                Tags=tags\n",
    "                               )\n",
    "    rts_dataset_arn = create_dataset_rts_response['DatasetArn']\n",
    "    status = util.wait(lambda: forecast.describe_dataset(DatasetArn=rts_dataset_arn))\n",
    "    assert status\n",
    "except forecast.exceptions.ResourceAlreadyExistsException:\n",
    "    rts_dataset_arn = f\"arn:aws:forecast:{region}:{account_id}:dataset/{rts_dataset_name}\"\n",
    "    print(f\"Related dataset {rts_dataset_arn} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_arns = []\n",
    "dataset_arns.append(tts_dataset_arn)\n",
    "dataset_arns.append(rts_dataset_arn)\n",
    "update_dataset_response = forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=dataset_arns)\n",
    "status = util.wait(lambda: forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Import data from S3 to Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Depending on the data size, the import can take 10 mins or more to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import_tts_dataset_response = True\n",
    "if len(util.get_dataset_import_jobs(tts_dataset_arn, forecast)) > 0:\n",
    "    print(\"Target dataset has already imported data.\")\n",
    "    import_tts_dataset_response = True if input(\"Re-import (y/N)? \").lower() == \"y\" else False\n",
    "\n",
    "if import_tts_dataset_response:\n",
    "    tts_dataset_import_job_response = \\\n",
    "        forecast.create_dataset_import_job(DatasetImportJobName=f\"tts_job_{datetime.now().strftime(date_format)}\",\n",
    "                                           DatasetArn=tts_dataset_arn,\n",
    "                                           DataSource={\n",
    "                                             \"S3Config\": {\n",
    "                                                 \"Path\": s3_target_data_key,\n",
    "                                                 \"RoleArn\": role_arn\n",
    "                                             }\n",
    "                                           },\n",
    "                                           TimestampFormat=DATASET_TIMESTAMP_FORMAT,\n",
    "                                           Tags=tags\n",
    "                                          )\n",
    "    tts_dataset_import_job_arn=tts_dataset_import_job_response['DatasetImportJobArn']\n",
    "    status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=tts_dataset_import_job_arn))\n",
    "    assert status\n",
    "    print(\"Target data imported.\")\n",
    "else:\n",
    "    print(\"Target data re-import skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Depending on the data size, the import can take 10 mins or more to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import_rts_dataset_response = True\n",
    "if len(util.get_dataset_import_jobs(rts_dataset_arn, forecast)) > 0:\n",
    "    print(\"Related dataset has already imported data.\")\n",
    "    import_rts_dataset_response = True if input(\"Re-import (y/N)? \").lower() == \"y\" else False\n",
    "\n",
    "if import_rts_dataset_response:\n",
    "    rts_dataset_import_job_response = \\\n",
    "        forecast.create_dataset_import_job(DatasetImportJobName=f\"rts_job_{datetime.now().strftime(date_format)}\",\n",
    "                                           DatasetArn=rts_dataset_arn,\n",
    "                                           DataSource= {\n",
    "                                             \"S3Config\" : {\n",
    "                                                 \"Path\": s3_related_data_key,\n",
    "                                                 \"RoleArn\": role_arn\n",
    "                                             } \n",
    "                                           },\n",
    "                                           TimestampFormat=DATASET_TIMESTAMP_FORMAT,\n",
    "                                           Tags=tags\n",
    "                                          )\n",
    "    rts_dataset_import_job_arn = rts_dataset_import_job_response[\"DatasetImportJobArn\"]\n",
    "    status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=rts_dataset_import_job_arn))\n",
    "    assert status\n",
    "    print(\"Related data imported.\")\n",
    "else:\n",
    "    print(\"Related data re-import skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algorithm_arn = 'arn:aws:forecast:::algorithm/'\n",
    "algorithm = 'Deep_AR_Plus'\n",
    "algorithm_arn_deep_ar_plus = algorithm_arn + algorithm\n",
    "predictor_name_deep_ar = f\"{dataset_group_name}_{algorithm.lower()}\"\n",
    "print(f\"Predictor Name = {predictor_name_deep_ar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Training a forecast model can take several hours to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrain_predictor_deep_ar = False\n",
    "existing_predictor_deep_ar = util.get_predictor(predictor_name_deep_ar, forecast)\n",
    "\n",
    "if existing_predictor_deep_ar:\n",
    "    predictor_arn_deep_ar = existing_predictor_deep_ar['PredictorArn']\n",
    "    print(f\"DeepAR+ Predictor {predictor_arn_deep_ar} already exists.\")\n",
    "    retrain_predictor_deep_ar = True if input(\"Retrain model (y/N)? \").lower() == \"y\" else False\n",
    "\n",
    "if existing_predictor_deep_ar and retrain_predictor_deep_ar:\n",
    "    util.delete_forecasts_by_predictor(predictor_arn_deep_ar, forecast)\n",
    "    print(f\"Deleting DeepAR+ Predictor {predictor_arn_deep_ar}...\")\n",
    "    util.wait_till_delete(lambda: forecast.delete_predictor(PredictorArn=predictor_arn_deep_ar))\n",
    "elif existing_predictor_deep_ar and not retrain_predictor_deep_ar:\n",
    "    print(f\"Keeping existing DeepAR+ Predictor {predictor_arn_deep_ar}.\")\n",
    "\n",
    "if not existing_predictor_deep_ar or retrain_predictor_deep_ar:\n",
    "    create_predictor_deep_ar_response = \\\n",
    "        forecast.create_predictor(PredictorName=predictor_name_deep_ar,\n",
    "                                  AlgorithmArn=algorithm_arn_deep_ar_plus,\n",
    "                                  ForecastHorizon=FORECAST_LENGTH,\n",
    "                                  PerformAutoML=False,\n",
    "                                  PerformHPO=False,\n",
    "                                  InputDataConfig={\n",
    "                                      \"DatasetGroupArn\": dataset_group_arn,\n",
    "                                      \"SupplementaryFeatures\": [\n",
    "                                          {\"Name\": \"holiday\",\n",
    "                                           \"Value\": \"US\"}],\n",
    "                                  },\n",
    "                                  FeaturizationConfig={\"ForecastFrequency\": DATASET_FREQUENCY},\n",
    "                                  Tags=tags\n",
    "                                 )\n",
    "    predictor_arn_deep_ar = create_predictor_deep_ar_response['PredictorArn']\n",
    "    print(f\"Creating DeepAR+ Predictor {predictor_arn_deep_ar}...\")\n",
    "    status = util.wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_deep_ar))\n",
    "    assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn = 'arn:aws:forecast:::algorithm/'\n",
    "algorithm = 'Prophet'\n",
    "algorithm_arn_prophet = algorithm_arn + algorithm\n",
    "predictor_name_prophet = f\"{dataset_group_name}_{algorithm.lower()}\"\n",
    "print(f\"Predictor Name = {predictor_name_prophet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Training a forecast model can take several hours to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_predictor_prophet = False\n",
    "existing_predictor_prophet = util.get_predictor(predictor_name_prophet, forecast)\n",
    "\n",
    "if existing_predictor_prophet:\n",
    "    predictor_arn_prophet = existing_predictor_prophet['PredictorArn']\n",
    "    print(f\"Prophet Predictor {predictor_arn_prophet} already exists.\")\n",
    "    retrain_predictor_prophet = True if input(\"Retrain model (y/N)? \").lower() == \"y\" else False\n",
    "\n",
    "if existing_predictor_prophet and retrain_predictor_prophet:\n",
    "    print(f\"Deleting Prophet Predictor {predictor_arn_prophet}...\")\n",
    "    util.wait_till_delete(lambda: forecast.delete_predictor(PredictorArn=predictor_arn_prophet))\n",
    "elif existing_predictor_prophet and not retrain_predictor_prophet:\n",
    "    print(f\"Keeping existing Prophet Predictor {predictor_arn_prophet}.\")\n",
    "\n",
    "if not existing_predictor_prophet or retrain_predictor_prophet:\n",
    "    create_predictor_response = \\\n",
    "        forecast.create_predictor(PredictorName=predictor_name_prophet,\n",
    "                                  AlgorithmArn=algorithm_arn_prophet,\n",
    "                                  ForecastHorizon=FORECAST_LENGTH,\n",
    "                                  PerformAutoML=False,\n",
    "                                  PerformHPO=False,\n",
    "                                  InputDataConfig= {\n",
    "                                      \"DatasetGroupArn\": dataset_group_arn,\n",
    "                                      \"SupplementaryFeatures\": [\n",
    "                                          {\"Name\": \"holiday\",\n",
    "                                           \"Value\": \"US\"}],\n",
    "                                  },\n",
    "                                  FeaturizationConfig={\"ForecastFrequency\": DATASET_FREQUENCY},\n",
    "                                  Tags=tags\n",
    "                                 )\n",
    "    predictor_arn_prophet = create_predictor_response['PredictorArn']\n",
    "    print(f\"Creating Prophet Predictor {predictor_arn_prophet}...\")\n",
    "    status = util.wait(lambda: forecast.describe_predictor(PredictorArn=predictor_arn_prophet))\n",
    "    assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_name_auto = f\"{dataset_group_name}_auto\"\n",
    "print(f\"Predictor Name = {predictor_name_auto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Training a forecast model can take several hours to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "existing_predictor_auto = util.get_predictor(predictor_name_auto, forecast)\n",
    "if existing_predictor_auto:\n",
    "    predictor_arn_auto = existing_predictor_auto['PredictorArn']\n",
    "    print(f\"Auto Predictor {predictor_arn_auto} already exists.\")\n",
    "    if input(\"Retrain model (y/N)? \").lower() == \"y\":\n",
    "        args = {\n",
    "            \"PredictorName\": f\"{predictor_name_auto}_retrain_{datetime.now().strftime(date_format)}\",\n",
    "            \"ReferencePredictorArn\": predictor_arn_auto\n",
    "        }\n",
    "    else:\n",
    "        args = {}\n",
    "else:\n",
    "    args = {\n",
    "        \"PredictorName\": predictor_name_auto,\n",
    "        \"ForecastHorizon\": FORECAST_LENGTH,\n",
    "        \"ForecastFrequency\": DATASET_FREQUENCY,\n",
    "        \"DataConfig\": {\n",
    "            \"DatasetGroupArn\": dataset_group_arn,\n",
    "            \"AdditionalDatasets\": [\n",
    "                {\n",
    "                    \"Name\": \"holiday\",\n",
    "                    \"Configuration\": {\n",
    "                        \"CountryCode\": [\"US\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"Tags\": tags\n",
    "    }\n",
    "\n",
    "if args:\n",
    "    create_predictor_auto_response = forecast.create_auto_predictor(**args)\n",
    "    predictor_arn_auto = create_predictor_auto_response['PredictorArn']\n",
    "    print(f\"{'Retraining existing' if existing_predictor_auto else 'Creating new'} Auto Predictor {predictor_arn_auto}...\")\n",
    "    status = util.wait(lambda: forecast.describe_auto_predictor(PredictorArn=predictor_arn_auto))\n",
    "    assert status\n",
    "else:\n",
    "    print(f\"Keeping existing Auto Predictor {predictor_arn_auto}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Predictor Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_deep_ar_plus = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_deep_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_prophet = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics_auto = forecast.get_accuracy_metrics(PredictorArn=predictor_arn_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_metrics(metric_response, predictor_name):\n",
    "    df = pd.DataFrame(metric_response['PredictorEvaluationResults']\n",
    "                 [0]['TestWindows'][0]['Metrics']['WeightedQuantileLosses'])\n",
    "    df['Predictor'] = predictor_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_ar_metrics = extract_summary_metrics(error_metrics_deep_ar_plus, \"DeepAR\")\n",
    "prophet_metrics = extract_summary_metrics(error_metrics_prophet, \"Prophet\")\n",
    "auto_metrics = extract_summary_metrics(error_metrics_auto, \"Auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([deep_ar_metrics, prophet_metrics, auto_metrics]) \\\n",
    "    .pivot(index='Quantile', columns='Predictor', values='LossValue').plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Creating a forecast can take up to an hour to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_arn_deep_ar = util.create_forecast(\n",
    "    *util.create_forecast_name(dataset_group_name,\n",
    "                               'deep_ar',\n",
    "                               date_format=date_format\n",
    "                              ),\n",
    "    predictor_arn_deep_ar,\n",
    "    forecast,\n",
    "    tags,\n",
    "    ui_date_format=ui_date_format,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Creating a forecast can take up to an hour to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_arn_prophet = util.create_forecast(\n",
    "    *util.create_forecast_name(dataset_group_name,\n",
    "                               'prophet',\n",
    "                               date_format=date_format\n",
    "                              ),\n",
    "    predictor_arn_prophet,\n",
    "    forecast,\n",
    "    tags,\n",
    "    ui_date_format=ui_date_format,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Creating a forecast can take up to an hour to become **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_arn_auto = util.create_forecast(\n",
    "    *util.create_forecast_name(dataset_group_name,\n",
    "                               'auto',\n",
    "                               date_format=date_format\n",
    "                              ),\n",
    "    predictor_arn_auto,\n",
    "    forecast,\n",
    "    tags,\n",
    "    ui_date_format=ui_date_format,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exact = pd.read_csv(s3_target_data_key, header=None, thousands=',')\n",
    "exact.columns = ['timestamp', 'item_id', target_column_name]\n",
    "exact = exact.loc[exact['item_id'] == item_id]\n",
    "exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exact = util.load_exact_sol(s3_target_data_key, item_id, target_col_name=target_column_name)\n",
    "exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "future_df = util.prepare_data(\n",
    "    bucket_name,\n",
    "    future_data_key,\n",
    "    \"%m/%d/%Y\",\n",
    "    target_column_name,\n",
    "    item_id,\n",
    "    fill_missing_values=FILL_MISSING_DATA,\n",
    "    use_bank_day=USE_BANK_DAY,\n",
    "    minimal=True)\n",
    "future_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecasts_deep_ar = util.query_forecasts(\n",
    "    util.get_relevant_forecasts(f\"{prefix}_{PROJECT}_\", DATA_VERSION, \"deep_ar\", forecast), item_id, forecastquery\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_deep_ar_dfs = util.query_results_to_dataframes(forecasts_deep_ar, fill_missing_values=FILL_MISSING_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot using bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in forecast_deep_ar_dfs:\n",
    "    auto_plot = util.plot_bokeh_forecasts(\n",
    "        forecast_deep_ar_dfs[key],\n",
    "        exact,\n",
    "        freq=f'1{DATASET_FREQUENCY}',\n",
    "        forecastHorizon=FORECAST_LENGTH,\n",
    "        time_back=30,\n",
    "        future=future_df,\n",
    "        target_col_name=target_column_name,\n",
    "        reverse=True,\n",
    "        title=f\"Stock Price Forecast for {item_id} (Auto v{key})\"\n",
    "    )\n",
    "    show(auto_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecasts_prophet = util.query_forecasts(\n",
    "    util.get_relevant_forecasts(f\"{prefix}_{PROJECT}_\", DATA_VERSION, \"prophet\", forecast), item_id, forecastquery\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_prophet_dfs = util.query_results_to_dataframes(forecasts_prophet, fill_missing_values=FILL_MISSING_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot using bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in forecast_prophet_dfs:\n",
    "    auto_plot = util.plot_bokeh_forecasts(\n",
    "        forecast_prophet_dfs[key],\n",
    "        exact,\n",
    "        freq=f'1{DATASET_FREQUENCY}',\n",
    "        forecastHorizon=FORECAST_LENGTH,\n",
    "        time_back=30,\n",
    "        future=future_df,\n",
    "        target_col_name=target_column_name,\n",
    "        reverse=True,\n",
    "        title=f\"Stock Price Forecast for {item_id} (Auto v{key})\"\n",
    "    )\n",
    "    show(auto_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecasts_auto = util.query_forecasts(\n",
    "    util.get_relevant_forecasts(f\"{prefix}_{PROJECT}_\", DATA_VERSION, \"auto\", forecast), item_id, forecastquery\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecast_auto_dfs = util.query_results_to_dataframes(forecasts_auto, fill_missing_values=FILL_MISSING_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot using bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in forecast_auto_dfs:\n",
    "    auto_plot = util.plot_bokeh_forecasts(\n",
    "        forecast_auto_dfs[key],\n",
    "        exact,\n",
    "        freq=f'1{DATASET_FREQUENCY}',\n",
    "        forecastHorizon=FORECAST_LENGTH,\n",
    "        time_back=30,\n",
    "        future=future_df,\n",
    "        target_col_name=target_column_name,\n",
    "        reverse=True,\n",
    "        title=f\"Stock Price Forecast for {item_id} (Auto v{key})\"\n",
    "    )\n",
    "    show(auto_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.delete_dataset_group(dataset_group_arn, forecast)\n",
    "# util.delete_dataset_group(\"arn:aws:forecast:us-east-1:275279264324:dataset-group/mac_training_rivn_forecast_2\", forecast)\n",
    "# util.delete_predictors(\"arn:aws:forecast:us-east-1:275279264324:dataset-group/mac_training_rivn_forecast_2\", forecast)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
